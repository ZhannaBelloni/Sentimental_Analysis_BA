{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting experiments for results of grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 15:52:23.356922: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "2023-12-06 15:52:23.357022: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "2023-12-06 15:52:23.357032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sentimental_hwglu import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ============================================================================\n",
      " |  running:  LSTM\n",
      " ============================================================================\n",
      " running model LSTM\n",
      " running grid_search for model: LSTM\n",
      " model default parameters:  {'scoring': 'accuracy', 'cv': 2, 'verbose': 3, 'dump_file_base': '/tmp/grid_search/grid_search_model'}\n",
      " grid_search parameters:    {'data_selector__verbose': [1], 'data_selector__dimension': ['reviews_no_stopwords', 'stamm_no_stop_punct'], 'vect__max_features': [500, 1000], 'lstm_sa__verbose': [4], 'lstm_sa__epoches': [2]}\n",
      " Running grid_search: LSTMPipeline\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "[LSTM_PIPELINE] setting lstm_sa_max_words to  500\n",
      "[SET PARAMS]  {'epoches': 2, 'verbose': 4, 'max_words': 500}\n",
      " [DataSelector] selecting params:  {'dimension': 'reviews_no_stopwords', 'verbose': 1}\n",
      "train (3750, 417)  ->  [[  0   0   0 ...   6 292 318]\n",
      " [  0   0   0 ...  42 109 371]\n",
      " [  0   0   0 ... 179 323 204]\n",
      " ...\n",
      " [  0   0   0 ... 125 107 179]\n",
      " [  0   0   0 ... 247 122 304]\n",
      " [  0   0   0 ...   1 302 102]] , max:  499\n",
      "validate (1250, 417)  ->  [[  0   0   0 ...   8  20 203]\n",
      " [  0   0   0 ... 115 135  61]\n",
      " [  0   0   0 ... 448  47  78]\n",
      " ...\n",
      " [  0   0   0 ...  63 249 228]\n",
      " [  0   0   0 ...  80  53  49]\n",
      " [  0   0   0 ... 482 388   1]] , max:  499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zibaldone/projects/ai/zbb/.venv/lib64/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3750 samples, validate on 1250 samples\n",
      "Epoch 1/2\n",
      "3750/3750 [==============================] - 24s 6ms/step - loss: 0.6464 - accuracy: 0.6400 - val_loss: 0.5235 - val_accuracy: 0.7592\n",
      "Epoch 2/2\n",
      "3750/3750 [==============================] - 21s 6ms/step - loss: 0.4768 - accuracy: 0.7853 - val_loss: 0.4479 - val_accuracy: 0.7992\n",
      " [DataSelector] selecting params:  {'dimension': 'reviews_no_stopwords', 'verbose': 1}\n",
      "[PREDICT] ================================ \n",
      "[PREDICT] try on  (5000, 396) , params:  {'max_words': 500, 'max_length': None, 'epoches': 2, 'verbose': 4}\n",
      "[PREDICT] [[0.12731722 0.87268275]\n",
      " [0.91708475 0.08291525]\n",
      " [0.27067336 0.72932667]\n",
      " ...\n",
      " [0.742919   0.25708094]\n",
      " [0.1059388  0.8940612 ]\n",
      " [0.34844032 0.65155965]] -1\n",
      "[PREDICT] [1 0 1 ... 0 1 1]\n",
      "[CV 1/2] END data_selector__dimension=reviews_no_stopwords, data_selector__verbose=1, lstm_sa__epoches=2, lstm_sa__verbose=4, vect__max_features=500;, score=0.789 total time=  51.5s\n",
      "[LSTM_PIPELINE] setting lstm_sa_max_words to  500\n",
      "[SET PARAMS]  {'epoches': 2, 'verbose': 4, 'max_words': 500}\n",
      " [DataSelector] selecting params:  {'dimension': 'reviews_no_stopwords', 'verbose': 1}\n",
      "train (3750, 395)  ->  [[  0   0   0 ... 223   1 168]\n",
      " [  0   0   0 ... 376 331   8]\n",
      " [  0   0   0 ... 260 250   2]\n",
      " ...\n",
      " [  0   0   0 ... 149 402  60]\n",
      " [  0   0   0 ... 180  50  12]\n",
      " [  0   0   0 ...   1   5  49]] , max:  499\n",
      "validate (1250, 395)  ->  [[  0   0   0 ... 257  21   1]\n",
      " [  0   0   0 ... 141 369 207]\n",
      " [  0   0   0 ...  11 380  16]\n",
      " ...\n",
      " [  0   0   0 ...   9   2 337]\n",
      " [  0   0   0 ...  47 206  34]\n",
      " [  0   0   0 ...  60  19   3]] , max:  499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zibaldone/projects/ai/zbb/.venv/lib64/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3750 samples, validate on 1250 samples\n",
      "Epoch 1/2\n",
      "3750/3750 [==============================] - 23s 6ms/step - loss: 0.6237 - accuracy: 0.6608 - val_loss: 0.5404 - val_accuracy: 0.7448\n",
      "Epoch 2/2\n",
      "3750/3750 [==============================] - 19s 5ms/step - loss: 0.4519 - accuracy: 0.7984 - val_loss: 0.4853 - val_accuracy: 0.7880\n",
      " [DataSelector] selecting params:  {'dimension': 'reviews_no_stopwords', 'verbose': 1}\n",
      "[PREDICT] ================================ \n",
      "[PREDICT] try on  (5000, 414) , params:  {'max_words': 500, 'max_length': None, 'epoches': 2, 'verbose': 4}\n",
      "[PREDICT] [[0.90597105 0.09402895]\n",
      " [0.9536485  0.04635147]\n",
      " [0.27562952 0.7243704 ]\n",
      " ...\n",
      " [0.9726318  0.02736819]\n",
      " [0.1368713  0.8631287 ]\n",
      " [0.12092362 0.87907636]] -1\n",
      "[PREDICT] [0 0 1 ... 0 1 1]\n",
      "[CV 2/2] END data_selector__dimension=reviews_no_stopwords, data_selector__verbose=1, lstm_sa__epoches=2, lstm_sa__verbose=4, vect__max_features=500;, score=0.794 total time=  49.4s\n",
      "[LSTM_PIPELINE] setting lstm_sa_max_words to  1000\n",
      "[SET PARAMS]  {'epoches': 2, 'verbose': 4, 'max_words': 1000}\n",
      " [DataSelector] selecting params:  {'dimension': 'reviews_no_stopwords', 'verbose': 1}\n",
      "train (3750, 530)  ->  [[  0   0   0 ...  45 593  69]\n",
      " [  0   0   0 ... 801 255 232]\n",
      " [  0   0   0 ...  40  33   2]\n",
      " ...\n",
      " [  0   0   0 ... 259 725  51]\n",
      " [  0   0   0 ...  98  34 662]\n",
      " [  0   0   0 ... 261 611 319]] , max:  999\n",
      "validate (1250, 530)  ->  [[  0   0   0 ... 156   4  31]\n",
      " [  0   0   0 ...  29  76  42]\n",
      " [  0   0   0 ... 462  12 494]\n",
      " ...\n",
      " [  0   0   0 ... 840 273   5]\n",
      " [  0   0   0 ... 480 143 627]\n",
      " [  0   0   0 ...  78  33 263]] , max:  999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zibaldone/projects/ai/zbb/.venv/lib64/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3750 samples, validate on 1250 samples\n",
      "Epoch 1/2\n",
      "3750/3750 [==============================] - 31s 8ms/step - loss: 0.6541 - accuracy: 0.6357 - val_loss: 0.5717 - val_accuracy: 0.7928\n",
      "Epoch 2/2\n",
      "3750/3750 [==============================] - 26s 7ms/step - loss: 0.4570 - accuracy: 0.8013 - val_loss: 0.4137 - val_accuracy: 0.8184\n",
      " [DataSelector] selecting params:  {'dimension': 'reviews_no_stopwords', 'verbose': 1}\n",
      "[PREDICT] ================================ \n",
      "[PREDICT] try on  (5000, 475) , params:  {'max_words': 1000, 'max_length': None, 'epoches': 2, 'verbose': 4}\n",
      "[PREDICT] [[0.11939523 0.8806048 ]\n",
      " [0.94719285 0.05280718]\n",
      " [0.38108099 0.618919  ]\n",
      " ...\n",
      " [0.7879264  0.21207356]\n",
      " [0.08255681 0.9174432 ]\n",
      " [0.82989544 0.17010461]] -1\n",
      "[PREDICT] [1 0 1 ... 0 1 0]\n",
      "[CV 1/2] END data_selector__dimension=reviews_no_stopwords, data_selector__verbose=1, lstm_sa__epoches=2, lstm_sa__verbose=4, vect__max_features=1000;, score=0.820 total time= 1.1min\n",
      "[LSTM_PIPELINE] setting lstm_sa_max_words to  1000\n",
      "[SET PARAMS]  {'epoches': 2, 'verbose': 4, 'max_words': 1000}\n",
      " [DataSelector] selecting params:  {'dimension': 'reviews_no_stopwords', 'verbose': 1}\n",
      "train (3750, 486)  ->  [[  0   0   0 ...   1  67 413]\n",
      " [  0   0   0 ... 615  11 120]\n",
      " [  0   0   0 ... 732 339 316]\n",
      " ...\n",
      " [  0   0   0 ... 604 433   4]\n",
      " [  0   0   0 ...  47  51  11]\n",
      " [  0   0   0 ...  69   1 545]] , max:  999\n",
      "validate (1250, 486)  ->  [[  0   0   0 ... 506   2 614]\n",
      " [  0   0   0 ... 511   1 859]\n",
      " [  0   0   0 ... 132  69 463]\n",
      " ...\n",
      " [  0   0   0 ... 119  60 228]\n",
      " [  0   0   0 ...  95 583  60]\n",
      " [  0   0   0 ... 115 163 411]] , max:  999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zibaldone/projects/ai/zbb/.venv/lib64/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3750 samples, validate on 1250 samples\n",
      "Epoch 1/2\n",
      "3750/3750 [==============================] - 27s 7ms/step - loss: 0.6550 - accuracy: 0.6293 - val_loss: 0.5593 - val_accuracy: 0.7744\n",
      "Epoch 2/2\n",
      "3750/3750 [==============================] - 24s 6ms/step - loss: 0.4903 - accuracy: 0.7931 - val_loss: 0.4304 - val_accuracy: 0.8096\n",
      " [DataSelector] selecting params:  {'dimension': 'reviews_no_stopwords', 'verbose': 1}\n",
      "[PREDICT] ================================ \n",
      "[PREDICT] try on  (5000, 525) , params:  {'max_words': 1000, 'max_length': None, 'epoches': 2, 'verbose': 4}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "base_dir = '/data/zibaldone/projects/ai/zbb/'\n",
    "dir_ = '/data/zibaldone/projects/ai/zbb/tests/'\n",
    "results = main.exec([\n",
    "    'imdb', 'grid_search', \n",
    "    '-o', os.path.join(base_dir, \"data\"),\n",
    "    '-p', os.path.join(dir_, \"test_grid_search.json\"),\n",
    "    '-m', 'LSTM',\n",
    "    '-s', '/tmp/grid_search/'\n",
    "    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 15:52:27.902675: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-12-06 15:52:27.902723: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-12-06 15:52:27.902763: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ld5065): /proc/driver/nvidia/version does not exist\n",
      "2023-12-06 15:52:27.903080: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2023-12-06 15:52:27.919991: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2097505000 Hz\n",
      "2023-12-06 15:52:27.928029: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28917b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-06 15:52:27.928077: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "/data/zibaldone/projects/ai/zbb/.venv/lib64/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/tmp/grid_search/grid_search_model_LSTMPipeline.pkl', 'br') as f:\n",
    "    estimator = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [DataSelector] selecting params:  {'dimension': 'reviews_no_stopwords', 'verbose': 1}\n",
      "[PREDICT] ================================ \n",
      "[PREDICT] try on  (1, 1) , params:  {'max_words': 500, 'max_length': None, 'epoches': 2, 'verbose': 4}\n",
      "[PREDICT] [[0.5029585 0.4970415]] -1\n",
      "[PREDICT] [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.predict([\"incredible horible film\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
