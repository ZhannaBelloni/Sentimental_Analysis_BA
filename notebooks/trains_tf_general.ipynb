{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentimentAnalysisModel\n",
    "\n",
    "In this notebook we use the `sentimental_hwglu.sa_model.SentimentAnalysisModel` class to train different models in a standardize method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import time\n",
    "\n",
    "import sentimental_hwglu.sa_model as sam\n",
    "import sentimental_hwglu.utils as util_sa\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sentimental_hwglu.utils import Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"/home/zhanna/bachelorarbeit/zbb/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_directory is None:\n",
    "    print(\"Project directory: \")\n",
    "    data_directory = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = Project(data_directory)\n",
    "df_imdb = util_sa.loadIMDBdataset(filename=project.csv_filename_extened)\n",
    "# df_imdb = df_imdb[:2000]\n",
    "split_precentage_tests=0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>emoticons</th>\n",
       "      <th>reviews_no_punctuation</th>\n",
       "      <th>length</th>\n",
       "      <th>words</th>\n",
       "      <th>sentences</th>\n",
       "      <th>positive_emoticons</th>\n",
       "      <th>negative_emoticons</th>\n",
       "      <th>stamm</th>\n",
       "      <th>stamm_no_punctuation</th>\n",
       "      <th>stamm_length</th>\n",
       "      <th>stamm_words</th>\n",
       "      <th>stamm_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>andreas arrives in a strange city . he doesn't...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>andreas arrives in a strange city he doesn't r...</td>\n",
       "      <td>1439</td>\n",
       "      <td>288</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>andrea arriv strang citi . rememb came got . o...</td>\n",
       "      <td>andrea arriv strang citi rememb came got order...</td>\n",
       "      <td>851</td>\n",
       "      <td>162</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>carrot top's \" chairman of the board \" and his...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>carrot top's chairman of the board and his at&amp;...</td>\n",
       "      <td>931</td>\n",
       "      <td>159</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>carrot top' \" chairman board \" hi at&amp;t commerc...</td>\n",
       "      <td>carrot top chairman board hi at&amp;t commerci liv...</td>\n",
       "      <td>639</td>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the bad out takes from \" reign of fire \" strun...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the bad out takes from reign of fire strung to...</td>\n",
       "      <td>187</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bad take \" reign fire \" strung togeth , withou...</td>\n",
       "      <td>bad take reign fire strung togeth without ani ...</td>\n",
       "      <td>131</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i saw the the bourne ultimatum last summer wit...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i saw the the bourne ultimatum last summer wit...</td>\n",
       "      <td>866</td>\n",
       "      <td>190</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>saw bourn ultimatum last summer friend , , wow...</td>\n",
       "      <td>saw bourn ultimatum last summer friend wow alr...</td>\n",
       "      <td>557</td>\n",
       "      <td>120</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>this is possibly the worst film i've ever seen...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is possibly the worst film i've ever seen...</td>\n",
       "      <td>3018</td>\n",
       "      <td>602</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>thi possibl worst film i'v ever seen . fact ha...</td>\n",
       "      <td>thi possibl worst film i'v ever seen fact ha f...</td>\n",
       "      <td>1852</td>\n",
       "      <td>351</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>49995</td>\n",
       "      <td>today , being president's day , my wife and i ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>today being president's day my wife and i had ...</td>\n",
       "      <td>2407</td>\n",
       "      <td>506</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>today , president' day , wife \" notebook \" dvd...</td>\n",
       "      <td>today presidentday wife notebook dvd check loc...</td>\n",
       "      <td>1506</td>\n",
       "      <td>318</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>49996</td>\n",
       "      <td>this is one of the worst movies i have ever se...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is one of the worst movies i have ever se...</td>\n",
       "      <td>204</td>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>thi one worst movi ever seen ! saw toronto fil...</td>\n",
       "      <td>thi one worst movi ever seen saw toronto film ...</td>\n",
       "      <td>135</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>49997</td>\n",
       "      <td>this is exactly the type of film that frustrat...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is exactly the type of film that frustrat...</td>\n",
       "      <td>1278</td>\n",
       "      <td>265</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>thi exactli type film frustrat . great cast , ...</td>\n",
       "      <td>thi exactli type film frustrat great cast grea...</td>\n",
       "      <td>807</td>\n",
       "      <td>163</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>49998</td>\n",
       "      <td>\" algie , the miner \" is one bad and unfunny ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algie the miner is one bad and unfunny silent...</td>\n",
       "      <td>663</td>\n",
       "      <td>132</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" algi , miner \" one bad unfunni silent comedi...</td>\n",
       "      <td>algi miner one bad unfunni silent comedi time...</td>\n",
       "      <td>448</td>\n",
       "      <td>88</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>49999</td>\n",
       "      <td>this movie is important to those of us interes...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this movie is important to those of us interes...</td>\n",
       "      <td>730</td>\n",
       "      <td>142</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>thi movi import us interest western histori be...</td>\n",
       "      <td>thi movi import us interest western histori be...</td>\n",
       "      <td>418</td>\n",
       "      <td>76</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                            reviews  sentiment  \\\n",
       "0          0  andreas arrives in a strange city . he doesn't...          1   \n",
       "1          1  carrot top's \" chairman of the board \" and his...          0   \n",
       "2          2  the bad out takes from \" reign of fire \" strun...          0   \n",
       "3          3  i saw the the bourne ultimatum last summer wit...          1   \n",
       "4          4  this is possibly the worst film i've ever seen...          0   \n",
       "...      ...                                                ...        ...   \n",
       "49995  49995  today , being president's day , my wife and i ...          0   \n",
       "49996  49996  this is one of the worst movies i have ever se...          0   \n",
       "49997  49997  this is exactly the type of film that frustrat...          0   \n",
       "49998  49998   \" algie , the miner \" is one bad and unfunny ...          0   \n",
       "49999  49999  this movie is important to those of us interes...          1   \n",
       "\n",
       "      emoticons                             reviews_no_punctuation  length  \\\n",
       "0           NaN  andreas arrives in a strange city he doesn't r...    1439   \n",
       "1           NaN  carrot top's chairman of the board and his at&...     931   \n",
       "2           NaN  the bad out takes from reign of fire strung to...     187   \n",
       "3           NaN  i saw the the bourne ultimatum last summer wit...     866   \n",
       "4           NaN  this is possibly the worst film i've ever seen...    3018   \n",
       "...         ...                                                ...     ...   \n",
       "49995       NaN  today being president's day my wife and i had ...    2407   \n",
       "49996       NaN  this is one of the worst movies i have ever se...     204   \n",
       "49997       NaN  this is exactly the type of film that frustrat...    1278   \n",
       "49998       NaN   algie the miner is one bad and unfunny silent...     663   \n",
       "49999       NaN  this movie is important to those of us interes...     730   \n",
       "\n",
       "       words  sentences  positive_emoticons  negative_emoticons  \\\n",
       "0        288         16                   0                   0   \n",
       "1        159          5                   0                   0   \n",
       "2         42          3                   0                   0   \n",
       "3        190          9                   0                   0   \n",
       "4        602         26                   0                   0   \n",
       "...      ...        ...                 ...                 ...   \n",
       "49995    506         21                   0                   0   \n",
       "49996     38          3                   0                   0   \n",
       "49997    265         14                   0                   0   \n",
       "49998    132          9                   0                   0   \n",
       "49999    142          6                   0                   0   \n",
       "\n",
       "                                                   stamm  \\\n",
       "0      andrea arriv strang citi . rememb came got . o...   \n",
       "1      carrot top' \" chairman board \" hi at&t commerc...   \n",
       "2      bad take \" reign fire \" strung togeth , withou...   \n",
       "3      saw bourn ultimatum last summer friend , , wow...   \n",
       "4      thi possibl worst film i'v ever seen . fact ha...   \n",
       "...                                                  ...   \n",
       "49995  today , president' day , wife \" notebook \" dvd...   \n",
       "49996  thi one worst movi ever seen ! saw toronto fil...   \n",
       "49997  thi exactli type film frustrat . great cast , ...   \n",
       "49998  \" algi , miner \" one bad unfunni silent comedi...   \n",
       "49999  thi movi import us interest western histori be...   \n",
       "\n",
       "                                    stamm_no_punctuation  stamm_length  \\\n",
       "0      andrea arriv strang citi rememb came got order...           851   \n",
       "1      carrot top chairman board hi at&t commerci liv...           639   \n",
       "2      bad take reign fire strung togeth without ani ...           131   \n",
       "3      saw bourn ultimatum last summer friend wow alr...           557   \n",
       "4      thi possibl worst film i'v ever seen fact ha f...          1852   \n",
       "...                                                  ...           ...   \n",
       "49995  today presidentday wife notebook dvd check loc...          1506   \n",
       "49996  thi one worst movi ever seen saw toronto film ...           135   \n",
       "49997  thi exactli type film frustrat great cast grea...           807   \n",
       "49998   algi miner one bad unfunni silent comedi time...           448   \n",
       "49999  thi movi import us interest western histori be...           418   \n",
       "\n",
       "       stamm_words  stamm_sentences  \n",
       "0              162               14  \n",
       "1              111                5  \n",
       "2               29                3  \n",
       "3              120                8  \n",
       "4              351               24  \n",
       "...            ...              ...  \n",
       "49995          318               17  \n",
       "49996           25                3  \n",
       "49997          163               11  \n",
       "49998           88                8  \n",
       "49999           76                6  \n",
       "\n",
       "[50000 rows x 15 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentimental_hwglu.sa_naive import NaiveSA\n",
    "from sentimental_hwglu.sa_afinn import AFinnPipeline\n",
    "from sentimental_hwglu.sa_afinn import VaderPipeline\n",
    "from sentimental_hwglu.sa_logistic_regression import LogisticRegressionTfid\n",
    "from sentimental_hwglu.sa_rnn import SA_LSTM_Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data, split it into train and test and feed the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = [\n",
    "    NaiveSA(verbose=True, weigth_added_words=0.0),\n",
    "    AFinnPipeline(),\n",
    "    VaderPipeline(),\n",
    "    LogisticRegressionTfid(),\n",
    "    SA_LSTM_Pipeline(max_words=1000, max_length=None, epochs=5),\n",
    "]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_imdb.reviews, df_imdb.sentiment, test_size=split_precentage_tests, random_state=42)\n",
    "for model in models:\n",
    "    print(\" Running pipeline: \" + str(model))\n",
    "    st = time.time()\n",
    "    r = sam.run_sentimental_analysis_pipeline(model, X_train, y_train, X_test, y_test)\n",
    "    et = time.time()\n",
    "    print(\" precision = {}, recall = {}, F1 = {}\".format(r.precision(), r.recall(), r.f1_score()))\n",
    "    results[str(model)] = r\n",
    "    print(' Execution time:', et - st, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, r in results.items():\n",
    "    print(\"------------------------------------\")\n",
    "    print(k, \":\")\n",
    "    print(\"    precision: \", r.precision())\n",
    "    print(\"    recall   : \", r.recall())\n",
    "    print(\"    F1       : \", r.f1_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __():\n",
    "    models = [\n",
    "        NaiveSA(verbose=True, weigth_added_words=0.0),\n",
    "        # AFinnPipeline(),\n",
    "        # VaderPipeline(),\n",
    "        LogisticRegressionTfid(),\n",
    "        SA_LSTM_Pipeline(max_words=1000, max_length=None, epochs=5),\n",
    "    ]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_imdb.reviews, df_imdb.sentiment, test_size=split_precentage_tests, random_state=42)\n",
    "    for model in models:\n",
    "        print(\" Running pipeline: \" + str(model))\n",
    "        st = time.time()\n",
    "        r = sam.run_sa_cross_validation_pipeline(model, X_train, y_train, X_test, y_test)\n",
    "        et = time.time()\n",
    "        print(\" precision = {}, recall = {}, F1 = {}\".format(r.precision(), r.recall(), r.f1_score()))\n",
    "        results[str(model)] = r\n",
    "        print(' Execution time:', et - st, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test to use GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_imdb.reviews, df_imdb.sentiment, test_size=split_precentage_tests, random_state=42)\n",
    "param_grid = [\n",
    "    {\"weigth_added_words\": [0.0]},\n",
    "    {\"weigth_added_words\": [0.5]},\n",
    "    {\"weigth_added_words\": [1.0]},\n",
    "    {\"weigth_added_words\": [2.0]},\n",
    "]\n",
    "\n",
    "pipeline = NaiveSA(verbose=True)\n",
    "gs = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanna/bachelorarbeit/zbb/venv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:821: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zhanna/bachelorarbeit/zbb/venv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 810, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/home/zhanna/bachelorarbeit/zbb/venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 266, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
      "  File \"/home/zhanna/bachelorarbeit/zbb/venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 353, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"/home/zhanna/bachelorarbeit/zbb/venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 86, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "  File \"/home/zhanna/bachelorarbeit/zbb/venv/lib/python3.10/site-packages/sklearn/utils/_response.py\", line 182, in _get_response_values\n",
      "    classes = estimator.classes_\n",
      "AttributeError: 'NaiveSA' object has no attribute 'classes_'\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " running set_words  1 / 2\n",
      " tokenization for review  [################### ] 99.8%\n",
      " tokenization took  1.6386289596557617  sec.\n",
      " running set_words  2 / 2\n",
      " tokenization for review  [##################  ] 93.5%\n",
      " tokenization took  1.4227676391601562  sec.\n",
      " create common words\n",
      " function executed in 0.1048s\n",
      " create only negative words\n",
      " function executed in 0.0442s\n",
      " create only positive words\n",
      " function executed in 0.0497s\n",
      " function executed in 3.2856s\n",
      " running set_words  1 / 2\n",
      " tokenization for review  [################### ] 99.8%\n",
      " tokenization took  1.4570131301879883  sec.\n",
      " running set_words  2 / 2\n",
      " tokenization for review  [##################  ] 93.5%\n",
      " tokenization took  1.4087939262390137  sec.\n",
      " create common words\n",
      " function executed in 0.0964s\n",
      " create only negative words\n",
      " function executed in 0.0477s\n",
      " create only positive words\n",
      " function executed in 0.0739s\n",
      " function executed in 3.1018s\n",
      " running set_words  1 / 2\n",
      " tokenization for review  [################### ] 99.8%\n",
      " tokenization took  1.5218894481658936  sec.\n",
      " running set_words  2 / 2\n",
      " tokenization for review  [##################  ] 93.5%\n",
      " tokenization took  1.3893177509307861  sec.\n",
      " create common words\n",
      " function executed in 0.0965s\n",
      " create only negative words\n",
      " function executed in 0.0451s\n",
      " create only positive words\n",
      " function executed in 0.0551s\n",
      " function executed in 3.1232s\n",
      " running set_words  1 / 2\n",
      " tokenization for review  [################### ] 99.8%\n",
      " tokenization took  1.5661914348602295  sec.\n",
      " running set_words  2 / 2\n",
      " tokenization for review  [##################  ] 93.5%\n",
      " tokenization took  1.4862847328186035  sec.\n",
      " create common words\n",
      " function executed in 0.2762s\n",
      " create only negative words\n",
      " function executed in 0.0475s\n",
      " create only positive words\n",
      " function executed in 0.0525s\n",
      " function executed in 3.4458s\n",
      " running set_words  1 / 2\n",
      " tokenization for review  [################### ] 99.8%\n",
      " tokenization took  1.482121229171753  sec.\n",
      " running set_words  2 / 2\n",
      " tokenization for review  [##################  ] 93.5%\n",
      " tokenization took  1.4071018695831299  sec.\n",
      " create common words\n",
      " function executed in 0.1058s\n",
      " create only negative words\n",
      " function executed in 0.0456s\n",
      " create only positive words\n",
      " function executed in 0.0533s\n",
      " function executed in 3.1102s\n",
      " running set_words  1 / 2\n",
      " tokenization for review  [################### ] 99.8%\n",
      " tokenization took  1.4592862129211426  sec.\n",
      " running set_words  2 / 2\n",
      " tokenization for review  [##################  ] 93.5%\n",
      " tokenization took  1.4978783130645752  sec.\n",
      " create common words\n",
      " function executed in 0.1065s\n",
      " create only negative words\n",
      " function executed in 0.0556s\n",
      " create only positive words\n",
      " function executed in 0.0645s\n",
      " function executed in 3.1999s\n",
      " running set_words  1 / 2\n",
      " tokenization for review  [################### ] 99.8%\n",
      " tokenization took  1.6509125232696533  sec.\n",
      " running set_words  2 / 2\n",
      " tokenization for review  [##################  ] 93.5%\n",
      " tokenization took  1.514969825744629  sec.\n",
      " create common words\n",
      " function executed in 0.3148s\n",
      " create only negative words\n",
      " function executed in 0.0543s\n",
      " create only positive words\n",
      " function executed in 0.0575s\n",
      " function executed in 3.6140s\n",
      " running set_words  1 / 2\n",
      " tokenization for review  [################### ] 99.8%\n",
      " tokenization took  1.51326584815979  sec.\n",
      " running set_words  2 / 2\n",
      " tokenization for review  [##################  ] 93.5%\n",
      " tokenization took  2.045802354812622  sec.\n",
      " create common words\n",
      " function executed in 0.1236s\n",
      " create only negative words\n",
      " function executed in 0.0473s\n",
      " create only positive words\n",
      " function executed in 0.0531s\n",
      " function executed in 3.8007s\n",
      " running set_words  1 / 2\n",
      " tokenization for review  [################### ] 99.8%\n",
      " tokenization took  1.3831679821014404  sec.\n",
      " running set_words  2 / 2\n",
      " tokenization for review  [##################  ] 93.5%\n",
      " tokenization took  1.3779020309448242  sec.\n",
      " create common words\n",
      " function executed in 0.0956s\n",
      " create only negative words\n",
      " function executed in 0.0441s\n",
      " create only positive words\n",
      " function executed in 0.0541s\n",
      " function executed in 2.9706s\n",
      " running set_words  1 / 2\n",
      " tokenization for review  [################### ] 99.8%\n",
      " tokenization took  1.5202369689941406  sec.\n",
      " running set_words  2 / 2\n",
      " tokenization for review  [##################  ] 93.5%\n",
      " tokenization took  1.3849921226501465  sec.\n",
      " create common words\n",
      " function executed in 0.1092s\n",
      " create only negative words\n",
      " function executed in 0.0463s\n",
      " create only positive words\n",
      " function executed in 0.0523s\n",
      " function executed in 3.1298s\n",
      " running set_words  1 / 2\n",
      " tokenization for review  [################### ] 99.8%\n",
      " tokenization took  1.5441629886627197  sec.\n",
      " running set_words  2 / 2\n",
      " tokenization for review  [##################  ] 93.5%\n",
      " tokenization took  1.5042662620544434  sec.\n",
      " create common words\n",
      " function executed in 0.2996s\n",
      " create only negative words\n",
      " function executed in 0.0473s\n",
      " create only positive words\n",
      " function executed in 0.0573s\n",
      " function executed in 3.4706s\n",
      " running set_words  1 / 2\n",
      " tokenization for review  [################### ] 99.8%\n",
      " tokenization took  1.5796968936920166  sec.\n",
      " running set_words  2 / 2\n",
      " tokenization for review  [##################  ] 93.5%\n",
      " tokenization took  1.4086949825286865  sec.\n",
      " create common words\n",
      " function executed in 0.0973s\n",
      " create only negative words\n",
      " function executed in 0.0428s\n",
      " create only positive words\n",
      " function executed in 0.0549s\n",
      " function executed in 3.1996s\n",
      " running set_words  1 / 2\n",
      " tokenization for review  [################### ] 99.8%\n",
      " tokenization took  1.4616918563842773  sec.\n",
      " running set_words  2 / 2\n",
      " tokenization for review  [##################  ] 93.5%\n",
      " tokenization took  1.3422245979309082  sec.\n",
      " create common words\n",
      " function executed in 0.0936s\n",
      " create only negative words\n",
      " function executed in 0.0426s\n",
      " create only positive words\n",
      " function executed in 0.0519s\n",
      " function executed in 3.0098s\n",
      " running set_words  1 / 2\n",
      " tokenization for review  [################### ] 99.8%\n",
      " tokenization took  1.9017765522003174  sec.\n",
      " running set_words  2 / 2\n",
      " tokenization for review  [##################  ] 93.5%\n",
      " tokenization took  1.4463937282562256  sec.\n",
      " create common words\n",
      " function executed in 0.0903s\n",
      " create only negative words\n",
      " function executed in 0.0450s\n",
      " create only positive words\n",
      " function executed in 0.0576s\n",
      " function executed in 3.5720s\n",
      " running set_words  1 / 2\n",
      " tokenization for review  [################### ] 99.8%\n",
      " tokenization took  1.9120399951934814  sec.\n",
      " running set_words  2 / 2\n",
      " tokenization for review  [##################  ] 93.5%\n",
      " tokenization took  1.561720371246338  sec.\n",
      " create common words\n",
      " function executed in 0.2940s\n",
      " create only negative words\n",
      " function executed in 0.0454s\n",
      " create only positive words\n",
      " function executed in 0.0565s\n",
      " function executed in 3.8856s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanna/bachelorarbeit/zbb/venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=NaiveSA(),\n",
       "             param_grid=[{&#x27;weigth_added_words&#x27;: [0.0]},\n",
       "                         {&#x27;weigth_added_words&#x27;: [0.5]},\n",
       "                         {&#x27;weigth_added_words&#x27;: [1.0]},\n",
       "                         {&#x27;weigth_added_words&#x27;: [2.0]}],\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=NaiveSA(),\n",
       "             param_grid=[{&#x27;weigth_added_words&#x27;: [0.0]},\n",
       "                         {&#x27;weigth_added_words&#x27;: [0.5]},\n",
       "                         {&#x27;weigth_added_words&#x27;: [1.0]},\n",
       "                         {&#x27;weigth_added_words&#x27;: [2.0]}],\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: NaiveSA</label><div class=\"sk-toggleable__content\"><pre>NaiveSA</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NaiveSA</label><div class=\"sk-toggleable__content\"><pre>NaiveSA</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=NaiveSA(),\n",
       "             param_grid=[{'weigth_added_words': [0.0]},\n",
       "                         {'weigth_added_words': [0.5]},\n",
       "                         {'weigth_added_words': [1.0]},\n",
       "                         {'weigth_added_words': [2.0]}],\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NaiveSA</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NaiveSA</label><div class=\"sk-toggleable__content\"><pre>NaiveSA</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NaiveSA(weigth_added_words=0.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weigth_added_words': 0.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
